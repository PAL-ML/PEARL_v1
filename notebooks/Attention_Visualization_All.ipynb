{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attention Visualization Pearl-all.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "53N4k0pj_9qL",
        "xwSB5jZki3Cj",
        "EPOaoszwatAN",
        "ywRlMokfoTpn",
        "TU8ItJyWppxr",
        "VDFpddLlpsQy",
        "21slhZGCqANb",
        "4W8ARJVqBJXs",
        "v66V4MR2YUVK",
        "MHsSslW0Q5Ny"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVwDdwy-UtLo"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "dc10ed7b-9fe9-4fcd-9584-432ccdcaa3ed"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA version: 11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBVr18E5tse8",
        "outputId": "9c4bd3ea-bf5b-4f8c-99f2-33a0ac055b92"
      },
      "source": [
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 1.5MB/s eta 0:03:43tcmalloc: large alloc 1147494400 bytes == 0x55ef911c2000 @  0x7fda11f66615 0x55ef572b0cdc 0x55ef5739052a 0x55ef572b3afd 0x55ef573a4fed 0x55ef57327988 0x55ef573224ae 0x55ef572b53ea 0x55ef573277f0 0x55ef573224ae 0x55ef572b53ea 0x55ef5732432a 0x55ef573a5e36 0x55ef57323853 0x55ef573a5e36 0x55ef57323853 0x55ef573a5e36 0x55ef57323853 0x55ef573a5e36 0x55ef574283e1 0x55ef573886a9 0x55ef572f3cc4 0x55ef572b4559 0x55ef573284f8 0x55ef572b530a 0x55ef573233b5 0x55ef573227ad 0x55ef572b53ea 0x55ef573233b5 0x55ef572b530a 0x55ef573233b5\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.2MB/s eta 0:01:25tcmalloc: large alloc 1434370048 bytes == 0x55efd5818000 @  0x7fda11f66615 0x55ef572b0cdc 0x55ef5739052a 0x55ef572b3afd 0x55ef573a4fed 0x55ef57327988 0x55ef573224ae 0x55ef572b53ea 0x55ef573277f0 0x55ef573224ae 0x55ef572b53ea 0x55ef5732432a 0x55ef573a5e36 0x55ef57323853 0x55ef573a5e36 0x55ef57323853 0x55ef573a5e36 0x55ef57323853 0x55ef573a5e36 0x55ef574283e1 0x55ef573886a9 0x55ef572f3cc4 0x55ef572b4559 0x55ef573284f8 0x55ef572b530a 0x55ef573233b5 0x55ef573227ad 0x55ef572b53ea 0x55ef573233b5 0x55ef572b530a 0x55ef573233b5\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 1.2MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x55f02b004000 @  0x7fda11f66615 0x55ef572b0cdc 0x55ef5739052a 0x55ef572b3afd 0x55ef573a4fed 0x55ef57327988 0x55ef573224ae 0x55ef572b53ea 0x55ef5732360e 0x55ef573224ae 0x55ef572b53ea 0x55ef5732360e 0x55ef573224ae 0x55ef572b53ea 0x55ef5732360e 0x55ef573224ae 0x55ef572b53ea 0x55ef5732360e 0x55ef573224ae 0x55ef572b53ea 0x55ef5732360e 0x55ef572b530a 0x55ef5732360e 0x55ef573224ae 0x55ef572b53ea 0x55ef5732432a 0x55ef573224ae 0x55ef572b53ea 0x55ef5732432a 0x55ef573224ae 0x55ef572b5a81\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 16kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9MB 230kB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=406b22a344f7e4249b7bd2255914f848d8cf36066112a576044f6412b51f9ece\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, ftfy\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFS29hnhlY4"
      },
      "source": [
        "!pip install --quiet git+https://github.com/Sri-vatsa/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "## Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGom156-i2kL"
      },
      "source": [
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toGtcd-Ji_MD"
      },
      "source": [
        "#@title\n",
        "\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPOaoszwatAN"
      },
      "source": [
        "## Atariari"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2FNq-MAQe9A"
      },
      "source": [
        "!rm -r atari_rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhmgN5HRZGPH"
      },
      "source": [
        "!git clone https://github.com/Sri-vatsa/atari-representation-learning.git atari_rl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv1Q6aZrZPMN"
      },
      "source": [
        "%cd atari_rl\n",
        "!pip install -r requirements.txt\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEg_GmBFaMtj"
      },
      "source": [
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpU4yjx3aVhQ"
      },
      "source": [
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCq07cZJeVSG"
      },
      "source": [
        "!pip install git+git://github.com/openai/baselines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6E9neGuNVek"
      },
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! unrar x Roms.rar\n",
        "! unzip ROMS.zip\n",
        "! python -m atari_py.import_roms /content/ROMS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2R3oDHPCuGg"
      },
      "source": [
        "## RAFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mia8cNwCvlS"
      },
      "source": [
        "!git clone https://github.com/princeton-vl/RAFT.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywRlMokfoTpn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU8ItJyWppxr"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMc1AXzBlhzm"
      },
      "source": [
        "import os\n",
        "import clip\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import skimage\n",
        "import cv2\n",
        "import math\n",
        "import argparse\n",
        "import sys\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "from atari_rl.atariari.benchmark.episodes import get_episodes\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDFpddLlpsQy"
      },
      "source": [
        "## Import clip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIT6FUzuU1MO"
      },
      "source": [
        "clip.available_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DulkDuQeVA9x"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "net = model.visual\n",
        "\n",
        "#model_resnet, preprocess = clip.load(\"RN50\", device=device, jit=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6cpiIFHp9N6"
      },
      "source": [
        "input_resolution = 224\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Setting up input images frames\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YNQhfaZZoKo"
      },
      "source": [
        "def get_episode_steps(env_name, steps, collect_mode=\"random_agent\"):\n",
        "\n",
        "  eps, _ = get_episodes(env_name=env_name, steps=steps, \n",
        "                            collect_mode=collect_mode, train_mode=\"dry_run\", color=True)\n",
        "  return eps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVLd10U_BO4A"
      },
      "source": [
        "def get_img_patches(img, num_patches):\n",
        "  img = img.permute(1, 2, 0)\n",
        "  M, N =  int(img.shape[0]//math.sqrt(num_patches)), int(img.shape[1]//math.sqrt(num_patches))\n",
        "  patch_list = []\n",
        "  for y in range(0,img.shape[1],N):\n",
        "    for x in range(0,img.shape[0],M):\n",
        "      tensor = img[x:x+M,y:y+N].permute(2, 0, 1)\n",
        "      patch_list.append(tensor)\n",
        "  return patch_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ0GwG3IL7sx"
      },
      "source": [
        "def pad_img(img):\n",
        "  orig_img = img.permute(1, 2, 0).numpy()\n",
        "  img_shape = (212, 160, 3)\n",
        "  padded_img = np.zeros(img_shape)\n",
        "  padded_img[1:-1, :, :] = orig_img\n",
        "  tensor = torch.from_numpy(padded_img).permute(2, 0, 1)\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEYlauOULLZ5"
      },
      "source": [
        "def process_img(img, input_resolution):\n",
        "\n",
        "  if input_resolution == \"full-image\":\n",
        "    return img\n",
        "\n",
        "  elif input_resolution == \"2x2patches\":\n",
        "    num_patches = 4\n",
        "    patch_list = get_img_patches(img, num_patches)\n",
        "    return torch.stack(patch_list)\n",
        "\n",
        "  elif input_resolution == \"4x4patches\":\n",
        "    num_patches = 16\n",
        "    img = pad_img(img)\n",
        "    patch_list = get_img_patches(img, num_patches)\n",
        "    return torch.stack(patch_list)\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Invalid input resolution... choose between full-image, 2x2patches, 4x4patches\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwqX-Rs8puXC"
      },
      "source": [
        "def get_selected_frames(game_data, input_resolution, num_frames=5, start=0, skip=4):\n",
        "  \n",
        "  selected_game_data = {}\n",
        "\n",
        "  for key in game_data.keys():\n",
        "    raw_data = game_data[key]\n",
        "    raw_data = raw_data[0] # take only the first episode data\n",
        "\n",
        "    selected_frames = []\n",
        "    selected_ids = [x for x in range(start, num_frames*skip, skip)]\n",
        "\n",
        "    for i in selected_ids:\n",
        "      img = process_img(raw_data[i], input_resolution)\n",
        "      selected_frames.append(img)\n",
        "\n",
        "    selected_game_data[key] = selected_frames\n",
        "  return selected_game_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3urwwnaoDrpF"
      },
      "source": [
        "def get_selected_frames_by_consecutive_pairs(game_data, input_resolution, num_frames=5, start=0, skip=4):\n",
        "  \n",
        "  selected_game_data = {}\n",
        "\n",
        "  for key in game_data.keys():\n",
        "    raw_data = game_data[key]\n",
        "    raw_data = raw_data[0] # take only the first episode data\n",
        "\n",
        "    selected_frames = []\n",
        "    selected_ids = [x for x in range(start, num_frames*skip, skip)]\n",
        "\n",
        "    for i in selected_ids:\n",
        "      img = process_img(raw_data[i], input_resolution)\n",
        "      next_img = process_img(raw_data[i+1], input_resolution)\n",
        "      selected_frames.append((img, next_img))\n",
        "\n",
        "    selected_game_data[key] = selected_frames\n",
        "  return selected_game_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykGwIIZhcexg"
      },
      "source": [
        "env_names = [\"BreakoutNoFrameskip-v4\", \"DemonAttackNoFrameskip-v4\", \"BoxingNoFrameskip-v4\"]\n",
        "num_steps = 500\n",
        "num_frames = 6\n",
        "start = 0 \n",
        "skip = 4\n",
        "input_resolution = \"full-image\"\n",
        "input_resolution1 = \"2x2patches\"\n",
        "input_resolution2 = \"4x4patches\"\n",
        "\n",
        "env_keys =  [x.replace(\"NoFrameskip-v4\", \"\") for x in env_names]\n",
        "all_eps = [get_episode_steps(env_name, num_steps) for env_name in env_names]\n",
        "game_data = dict(zip(env_keys, all_eps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I093LPzvKBv1"
      },
      "source": [
        "selected_game_frames_full = get_selected_frames(game_data, input_resolution, num_frames=num_frames, start=start, skip=skip)\n",
        "selected_game_frames_2x2patches = get_selected_frames(game_data, input_resolution1, num_frames=num_frames, start=start, skip=skip)\n",
        "selected_game_frames_4x4patches = get_selected_frames(game_data, input_resolution2, num_frames=num_frames, start=start, skip=skip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeRmBQKnNQWp"
      },
      "source": [
        "# Viz 1: Get Attention map from CLIP visual transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrrTqkQKVO-c"
      },
      "source": [
        "def get_image_features(images):\n",
        "  image_input = torch.tensor(np.stack(images)).cuda()\n",
        "  image_input -= image_mean[:, None, None]\n",
        "  image_input /= image_std[:, None, None]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    image_features, attn_weights = model.encode_image(image_input)\n",
        "  \n",
        "  image_features = image_features.float()\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return image_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cl91kH-afev"
      },
      "source": [
        "def get_attention_map(img_tensor, original_img, model, get_mask=False):\n",
        "    image_input = torch.tensor(np.stack(img_tensor), dtype=torch.float16).cuda()\n",
        "    image_input -= image_mean[:, None, None]\n",
        "    image_input /= image_std[:, None, None]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      image_features, att_mat = model(image_input)\n",
        "\n",
        "    #att_mat = torch.stack(att_mat).squeeze(1)\n",
        "    #print(\"attention output shape: {}\".format(att_mat.shape))\n",
        "\n",
        "    # Average the attention weights across all heads.\n",
        "    #att_mat = torch.mean(att_mat, dim=1)\n",
        "    #print(\"ave attention output shape: {}\".format(att_mat.shape))\n",
        "\n",
        "    # To account for residual connections, we add an identity matrix to the\n",
        "    # attention matrix and re-normalize the weights.\n",
        "    residual_att = torch.eye(att_mat.size(1))\n",
        "    aug_att_mat = att_mat + residual_att.cuda()\n",
        "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
        "    #print(\"aug attention output shape: {}\".format(aug_att_mat.shape))\n",
        "\n",
        "    # Recursively multiply the weight matrices\n",
        "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
        "    joint_attentions[0] = aug_att_mat[0]\n",
        "\n",
        "    for n in range(1, aug_att_mat.size(0)):\n",
        "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
        "    \n",
        "    #print(\"joint attention output shape: {}\".format(joint_attentions.shape))\n",
        "\n",
        "    v = joint_attentions[-1]\n",
        "    #print(\"v: {}\".format(v.shape))\n",
        "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
        "    #print(\"grid size: {}\".format(grid_size))\n",
        "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
        "    #print(mask.shape)\n",
        "    if get_mask:\n",
        "        result = cv2.resize(mask / mask.max(), original_img.size)\n",
        "    else:\n",
        "        mask = cv2.resize(mask / mask.max(), original_img.size)[..., np.newaxis]\n",
        "        result = (mask * original_img).astype(\"uint8\")\n",
        "    #print(\"resulting attention output shape: {}\".format(result.shape))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2fwmgweZkaB"
      },
      "source": [
        "def plot_attention_map_full(original_img, att_map):\n",
        "    plt.figure()\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original')\n",
        "    ax2.set_title('Attention Map')\n",
        "    _ = ax1.imshow(original_img)\n",
        "    _ = ax2.imshow(att_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okBqusHgURIp"
      },
      "source": [
        "def plot_attention_map_2x2patches(original_patches, att_maps, num_patches):\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    fig, axes = plt.subplots(2, num_patches, figsize=(16, 8))\n",
        "\n",
        "    # original patch\n",
        "    for i in range(num_patches):\n",
        "      axes[0, i].set_title('Patch {}'.format(i))\n",
        "      axes[0, i].imshow(original_patches[i])\n",
        "\n",
        "      axes[1, i].set_title('Attention {}'.format(i))\n",
        "      axes[1, i].imshow(att_maps[i])\n",
        "\n",
        "    #fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HejEBNPc1i-s"
      },
      "source": [
        "def plot_attention_map_4x4patches(original_patches, att_maps, num_patches):\n",
        "\n",
        "    plt.figure(figsize=(32, 3))\n",
        "    fig, axes = plt.subplots(2, num_patches, figsize=(32, 3))\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.4)\n",
        "\n",
        "    # original patch\n",
        "    for i in range(num_patches):\n",
        "      axes[0, i].set_title('Patch {}'.format(i))\n",
        "      axes[0, i].imshow(original_patches[i], aspect='auto')\n",
        "\n",
        "      axes[1, i].set_title('Attention {}'.format(i))\n",
        "      axes[1, i].imshow(att_maps[i], aspect='auto')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsdYnevPyuit"
      },
      "source": [
        "def plot_attention_full(selected_game_frames, game):\n",
        "  frames = selected_game_frames[game]\n",
        "  for frame in frames:\n",
        "    plot_img = Image.fromarray(frame.permute(1, 2, 0).numpy())\n",
        "    \n",
        "    clip_inp = preprocess(plot_img)\n",
        "    result = get_attention_map(clip_inp.unsqueeze(0), plot_img, net)\n",
        "    plot_attention_map_full(plot_img, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qacsmqs3IAG3"
      },
      "source": [
        "def plot_attention_patches(selected_game_frames, game, num_patches):\n",
        "  frames = selected_game_frames[game]\n",
        "  for frame in frames:\n",
        "    patches = frame.split(split_size=1, dim=0)\n",
        "    \n",
        "    attn_maps, original_patches = [], []\n",
        "    for patch in patches:\n",
        "      patch = patch.squeeze(0)\n",
        "      plot_img = Image.fromarray(patch.permute(1, 2, 0).numpy().astype(np.uint8))\n",
        "      clip_inp = preprocess(plot_img)\n",
        "      result = get_attention_map(clip_inp.unsqueeze(0), plot_img, net)\n",
        "\n",
        "      attn_maps.append(result)\n",
        "      original_patches.append(plot_img)\n",
        "\n",
        "    if num_patches == 4:\n",
        "      plot_attention_map_2x2patches(original_patches, attn_maps, num_patches)\n",
        "    elif num_patches == 16:\n",
        "      plot_attention_map_4x4patches(original_patches, attn_maps, num_patches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmv1vUznHJlR"
      },
      "source": [
        "def plot_attention_for_game(selected_frames, game, input_resolution):\n",
        "  if input_resolution == \"full-image\":\n",
        "    plot_attention_full(selected_frames, game)\n",
        "  elif input_resolution == \"2x2patches\":\n",
        "    num_patches = 4\n",
        "    plot_attention_patches(selected_frames, game, num_patches)\n",
        "  elif input_resolution == \"4x4patches\":\n",
        "    num_patches = 16\n",
        "    plot_attention_patches(selected_frames, game, num_patches)\n",
        "  else:\n",
        "    raise Exception(\"Invalid input resolution... choose between full-image, 2x2patches, 4x4patches\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKpEbzGn0Zvs"
      },
      "source": [
        "## Visualize attention maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2UqlDs3YP4L"
      },
      "source": [
        "### Full img"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ5faVTGywCL"
      },
      "source": [
        "games = list(selected_game_frames_full.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl3vgDSSy54I"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_full, games[0], input_resolution) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7RWL6UozLCH"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_full, games[1], input_resolution) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6PYaExFzLle"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_full, games[2], input_resolution) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v66V4MR2YUVK"
      },
      "source": [
        "### 2x2 patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbr-xF8DYUVK"
      },
      "source": [
        "games = list(selected_game_frames_2x2patches.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZiLwVIeYUVL"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_2x2patches, games[0], input_resolution1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRxg9Q_RYUVL"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_2x2patches, games[1], input_resolution1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgEB2KKXYUVM"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_2x2patches, games[2], input_resolution1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVvgtknAYV-S"
      },
      "source": [
        "### 4x4 patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyL4Ic_dYV-T"
      },
      "source": [
        "games = list(selected_game_frames_4x4patches.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw6oAwscYV-T"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_4x4patches, games[0], input_resolution2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNUgEqqFYV-T"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_4x4patches, games[1], input_resolution2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKYOjtm-YV-U"
      },
      "source": [
        "plot_attention_for_game(selected_game_frames_4x4patches, games[2], input_resolution2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLLLO-I3HQPq"
      },
      "source": [
        "# Viz2: Get optical flow masks from RAFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL7_pG5gHV-o"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-HpKzrmHVDU"
      },
      "source": [
        "def flow_to_mask(flow_uv, mask_type=\"norm\", clip_flow=None, convert_to_bgr=False):\n",
        "    assert flow_uv.ndim == 3, 'input flow must have three dimensions'\n",
        "    assert flow_uv.shape[2] == 2, 'input flow must have shape [H,W,2]'\n",
        "    if clip_flow is not None:\n",
        "        flow_uv = np.clip(flow_uv, 0, clip_flow)\n",
        "    u = flow_uv[:,:,0]\n",
        "    v = flow_uv[:,:,1]\n",
        "    rad = np.sqrt(np.square(u) + np.square(v))\n",
        "\n",
        "    if mask_type == \"norm\":\n",
        "        mask = rad / np.max(rad)\n",
        "    elif mask_type == \"clip\":\n",
        "        mask = np.clip(rad, 0, 1)\n",
        "\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1t6b7PVHbtg"
      },
      "source": [
        "def mask_image(image, mask, add_background_noise=False):\n",
        "    mask_3c = np.stack([mask for _ in range(3)])\n",
        "    inv_mask_3c = 1 - mask_3c\n",
        "\n",
        "    masked_image = image * mask_3c\n",
        "\n",
        "    if add_background_noise:\n",
        "        if np.max(image) > 1:\n",
        "            noise = np.random.randint(0, np.max(image), masked_image.shape)\n",
        "        else:\n",
        "            noise = np.random.random(masked_image.shape)\n",
        "        masked_image = masked_image + noise*inv_mask_3c\n",
        "    \n",
        "    return masked_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmxyKuosKB6E"
      },
      "source": [
        "def get_optical_flow_from_consecutive_images(\n",
        "    image1, \n",
        "    image2, \n",
        "    model, \n",
        "    output_types=[\"mask\"]\n",
        "):\n",
        "    out = []\n",
        "\n",
        "    flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n",
        "    \n",
        "    if \"output\" in output_types:\n",
        "        flow_output = flow_viz.flow_to_image(flow_up[0].permute(1,2,0).cpu().numpy())\n",
        "        flow_output_im = flow_output.astype(np.uint8)\n",
        "        out.append(flow_output_im)\n",
        "    if \"mask\" in output_types:\n",
        "        flow_numpy = flow_up[0].permute(1,2,0).cpu().numpy()\n",
        "        image_numpy = image1[0].cpu().numpy()\n",
        "\n",
        "        mask = flow_to_mask(flow_numpy)\n",
        "        masked_image = mask_image(image_numpy, mask)\n",
        "        masked_image_im = masked_image.astype(np.uint8).transpose(1,2,0)\n",
        "        \n",
        "        out.append(masked_image_im)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN2q_ANpOfPb"
      },
      "source": [
        "def preprocess_for_raft(im1, im2):\n",
        "    image1 = im1.float().unsqueeze(0).cuda()\n",
        "    image2 = im2.float().unsqueeze(0).cuda()\n",
        "    padder = InputPadder(image1.shape)\n",
        "    image1, image2 = padder.pad(image1, image2)\n",
        "\n",
        "    return image1, image2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPY4bxUoHd10"
      },
      "source": [
        "def plot_optical_flow_output_full(images_to_plot, titles):\n",
        "    fig, axes = plt.subplots(ncols=len(images_to_plot), figsize=(24, 16))\n",
        "\n",
        "    for idx_ax, ax in enumerate(axes):\n",
        "        ax.set_title(titles[idx_ax])\n",
        "        _ = ax.imshow(images_to_plot[idx_ax])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3k4SNSaafuq"
      },
      "source": [
        "def plot_optical_flow_output_2x2patches(lists_to_plot, titles_to_plot, num_patches):\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    fig, axes = plt.subplots(len(lists_to_plot), num_patches, figsize=(16, 12))\n",
        "\n",
        "    # original patch\n",
        "    for i in range(num_patches):\n",
        "        for idx_l in range(len(lists_to_plot)):\n",
        "            axes[idx_l, i].set_title('{} {}'.format(titles_to_plot[idx_l], i))\n",
        "            axes[idx_l, i].imshow(lists_to_plot[idx_l][i])\n",
        "\n",
        "    #fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T95lHpcvafu1"
      },
      "source": [
        "def plot_optical_flow_output_4x4patches(lists_to_plot, titles_to_plot, num_patches):\n",
        "\n",
        "    plt.figure(figsize=(32, 5))\n",
        "    fig, axes = plt.subplots(len(lists_to_plot), num_patches, figsize=(32, 5))\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.4)\n",
        "\n",
        "    # original patch\n",
        "    for i in range(num_patches):\n",
        "        for idx_l in range(len(lists_to_plot)):\n",
        "            axes[idx_l, i].set_title('{} {}'.format(titles_to_plot[idx_l], i))\n",
        "            axes[idx_l, i].imshow(lists_to_plot[idx_l][i], aspect='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgVju6bbL5PB"
      },
      "source": [
        "def plot_optical_flow_full(\n",
        "    selected_game_frames, \n",
        "    game, \n",
        "    model,\n",
        "    flow_output_types = [\"mask\"]\n",
        "):\n",
        "    frames = selected_game_frames[game]\n",
        "    with torch.no_grad():\n",
        "        for frame in frames:\n",
        "            images_to_plot = []\n",
        "            titles_to_plot = []\n",
        "            \n",
        "            im1, im2 = frame\n",
        "            image1, image2 = preprocess_for_raft(im1, im2)\n",
        "\n",
        "            plot_im = image1[0].permute(1,2,0).cpu().numpy().astype(np.uint8)\n",
        "            images_to_plot.append(plot_im)\n",
        "            titles_to_plot.append(\"Original\")\n",
        "\n",
        "            flow_outputs = get_optical_flow_from_consecutive_images(image1, image2, model, flow_output_types)\n",
        "            images_to_plot += flow_outputs\n",
        "\n",
        "            if \"output\" in flow_output_types:\n",
        "                titles_to_plot.append(\"Optical Flow Output\")\n",
        "            if \"mask\" in flow_output_types:\n",
        "                titles_to_plot.append(\"Optical Flow Mask\")\n",
        "\n",
        "            plot_optical_flow_output_full(images_to_plot, titles_to_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWRSlZ5MT-zP"
      },
      "source": [
        "def plot_optical_flow_patches(\n",
        "    selected_game_frames,\n",
        "    game,\n",
        "    model,\n",
        "    num_patches,\n",
        "    flow_output_types = [\"mask\"]\n",
        "):\n",
        "    frames = selected_game_frames[game]\n",
        "    with torch.no_grad():\n",
        "        for frame in frames:\n",
        "            im1, im2 = frame\n",
        "            patches1 = im1.split(split_size=1, dim=0)\n",
        "            patches2 = im2.split(split_size=1, dim=0)\n",
        "            \n",
        "            titles_to_plot = [\"Patch\"]\n",
        "            if \"output\" in flow_output_types:\n",
        "                titles_to_plot.append(\"OF Output\")\n",
        "            if \"mask\" in flow_output_types:\n",
        "                titles_to_plot.append(\"OF Mask\")\n",
        "\n",
        "\n",
        "            lists_to_plot = [[]] + [[] for _ in flow_output_types]\n",
        "            for idx_p in range(num_patches):\n",
        "                scale = np.sqrt(num_patches)\n",
        "                \n",
        "                p1 = patches1[idx_p]\n",
        "                p2 = patches2[idx_p]\n",
        "\n",
        "                # resize images to original size to avoid nans\n",
        "                p1 = F.interpolate(p1, scale_factor=scale).squeeze(0)\n",
        "                p2 = F.interpolate(p2, scale_factor=scale).squeeze(0)\n",
        "\n",
        "                patch1, patch2 = preprocess_for_raft(p1, p2)\n",
        "\n",
        "                plot_im = patch1[0].permute(1,2,0).cpu().numpy().astype(np.uint8)\n",
        "                lists_to_plot[0].append(plot_im)\n",
        "\n",
        "                flow_outputs = get_optical_flow_from_consecutive_images(\n",
        "                    patch1, \n",
        "                    patch2, \n",
        "                    model, \n",
        "                    flow_output_types\n",
        "                    )\n",
        "                for idx_o, out in enumerate(flow_outputs):\n",
        "                    lists_to_plot[idx_o+1].append(out)\n",
        "\n",
        "            if num_patches == 4:\n",
        "                plot_optical_flow_output_2x2patches(lists_to_plot, titles_to_plot, num_patches)\n",
        "            elif num_patches == 16:\n",
        "                plot_optical_flow_output_4x4patches(lists_to_plot, titles_to_plot, num_patches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLoNmqQRI_yh"
      },
      "source": [
        "def plot_optical_flow_for_game(\n",
        "    selected_frames, \n",
        "    game, \n",
        "    raft_args,\n",
        "    input_resolution,\n",
        "    flow_output_type = [\"mask\"]\n",
        "    ):\n",
        "\n",
        "    model = torch.nn.DataParallel(RAFT(raft_args))\n",
        "    model.load_state_dict(torch.load(raft_args.model))\n",
        "    model = model.module\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    if input_resolution == \"full-image\":\n",
        "        plot_optical_flow_full(selected_frames, game, model, flow_output_type)\n",
        "    elif input_resolution == \"2x2patches\":\n",
        "        num_patches = 4\n",
        "        plot_optical_flow_patches(selected_frames, game, model, num_patches, flow_output_type)\n",
        "    elif input_resolution == \"4x4patches\":\n",
        "        num_patches = 16\n",
        "        plot_optical_flow_patches(selected_frames, game, model, num_patches, flow_output_type)\n",
        "    else:\n",
        "        raise Exception(\"Invalid input resolution... choose between full-image, 2x2patches, 4x4patches\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHsSslW0Q5Ny"
      },
      "source": [
        "## RAFT initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxJ25EMOYie"
      },
      "source": [
        "% cd RAFT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W95fnscGOi72"
      },
      "source": [
        "!wget https://www.dropbox.com/s/4j4z58wuv8o0mfz/models.zip\n",
        "!unzip models.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQFULs39Llqy"
      },
      "source": [
        "sys.path.append('core')\n",
        "\n",
        "from utils import flow_viz\n",
        "from raft import RAFT\n",
        "from utils.utils import InputPadder\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AdpnRqDQ1o8"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model', default=\"models/raft-things.pth\", help=\"restore checkpoint\")\n",
        "# parser.add_argument('--path', default=\"demo-frames\", help=\"dataset for evaluation\")\n",
        "parser.add_argument('--small', action='store_true', help='use small model')\n",
        "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
        "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
        "raft_args = parser.parse_args(args=[\"--model=models/raft-things.pth\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msrGXdD0QM9g"
      },
      "source": [
        "## Visualize optical flow masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK7NN8WNQRAP"
      },
      "source": [
        "of_selected_game_frames_full = get_selected_frames_by_consecutive_pairs(game_data, input_resolution, num_frames=num_frames, start=start, skip=skip)\n",
        "of_selected_game_frames_2x2patches = get_selected_frames_by_consecutive_pairs(game_data, input_resolution1, num_frames=num_frames, start=start, skip=skip)\n",
        "of_selected_game_frames_4x4patches = get_selected_frames_by_consecutive_pairs(game_data, input_resolution2, num_frames=num_frames, start=start, skip=skip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpf5Fv5hxXq5"
      },
      "source": [
        "flow_output_types = [\"output\", \"mask\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k6nX6-4d7xW"
      },
      "source": [
        "### Full img"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EORFUtbLQgf1"
      },
      "source": [
        "of_games = list(of_selected_game_frames_full.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avSLEINHQnAo"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_full, \n",
        "    of_games[0],\n",
        "    raft_args,\n",
        "    input_resolution,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt5nP9ouxNCx"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_full, \n",
        "    of_games[1],\n",
        "    raft_args,\n",
        "    input_resolution,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1byE8BxExNVy"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_full, \n",
        "    of_games[2],\n",
        "    raft_args,\n",
        "    input_resolution,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GVi8hA1d5QG"
      },
      "source": [
        "### 2x2 patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oh82B8Nb325"
      },
      "source": [
        "of_games = list(of_selected_game_frames_2x2patches.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooM7_ojdS3tv"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_2x2patches, \n",
        "    of_games[0],\n",
        "    raft_args,\n",
        "    input_resolution1,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuhEyaVlqoAH"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_2x2patches, \n",
        "    of_games[1],\n",
        "    raft_args,\n",
        "    input_resolution1,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsXah5MkxlBf"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_2x2patches, \n",
        "    of_games[2],\n",
        "    raft_args,\n",
        "    input_resolution1,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRFEAzmdw3Fy"
      },
      "source": [
        "### 4x4 patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADTFtxytw3F2"
      },
      "source": [
        "of_games = list(of_selected_game_frames_4x4patches.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k0QcI0nw3F2"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_4x4patches, \n",
        "    of_games[0],\n",
        "    raft_args,\n",
        "    input_resolution2,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBugN8Zjw6-n"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_4x4patches, \n",
        "    of_games[1],\n",
        "    raft_args,\n",
        "    input_resolution2,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR6o4L2_xuRJ"
      },
      "source": [
        "plot_optical_flow_for_game(\n",
        "    of_selected_game_frames_4x4patches, \n",
        "    of_games[2],\n",
        "    raft_args,\n",
        "    input_resolution2,\n",
        "    flow_output_types\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkQO0F2Kxvrk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxONvvUo8dho"
      },
      "source": [
        "# Viz3 : Dino, efficientDet and image diff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT6gquQj8i6y"
      },
      "source": [
        "games = list(selected_game_frames_full.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhPk_-dG-jex"
      },
      "source": [
        "## Image diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQjUsm0H-053"
      },
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPK7UtSa-1yN"
      },
      "source": [
        "def image_diff(img1,img2, img1_nop):\n",
        "  # load the two input images\n",
        "  imageA = img1\n",
        "  imageB = img2\n",
        "  # convert the images to grayscale\n",
        "  grayA = cv2.cvtColor(imageA, cv2.COLOR_RGB2GRAY)\n",
        "  grayB = cv2.cvtColor(imageB, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "  # compute the Structural Similarity Index (SSIM) between the two\n",
        "  # images, ensuring that the difference image is returned\n",
        "  (score, diff) = ssim(grayA, grayB, full=True)\n",
        "  diff = (diff*255).astype(\"uint8\")\n",
        "\n",
        "  # threshold the difference image, followed by finding contours to\n",
        "  # obtain the regions of the two input images that differ\n",
        "  thresh = cv2.threshold(diff, 0, 1,\n",
        "    cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "  cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
        "    cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cnts = imutils.grab_contours(cnts)\n",
        "\n",
        "  maskkkkk = np.zeros((210,160), dtype=\"uint8\")\n",
        "\n",
        "  # loop over the contours\n",
        "  for c in cnts:\n",
        "    (x, y, w, h) = cv2.boundingRect(c)\n",
        "    cv2.rectangle(maskkkkk, (x, y), (x + w, y + h), 1, -1)\n",
        "\n",
        "  new_list = [maskkkkk,maskkkkk,maskkkkk]\n",
        "  stacked_thresh = np.stack(new_list)\n",
        "  final_masked = torch.from_numpy(stacked_thresh * img1_nop)\n",
        "\n",
        "  if score == 1:\n",
        "    final_masked = torch.from_numpy(img1_nop)\n",
        "\n",
        "  plt.figure()\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "  ax1.set_title('Original')\n",
        "  ax2.set_title('Image Diff')\n",
        "  _ = ax1.imshow(img1)\n",
        "  _ = ax2.imshow(final_masked.permute(1,2,0).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v1FSl6s-52K"
      },
      "source": [
        "for game in games:\n",
        "  for i in range(len(selected_game_frames_full[game])):\n",
        "    if i+1 < len(selected_game_frames_full[game]):\n",
        "      img1 = selected_game_frames_full[game][i].permute(1,2,0).numpy()\n",
        "      img2 = selected_game_frames_full[game][i+1].permute(1,2,0).numpy()\n",
        "      img3 = selected_game_frames_full[game][i].numpy()\n",
        "      image_diff(img1,img2,img3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JklzrX00-Cnz"
      },
      "source": [
        "## EfficientDet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT3FpRJWLxmy"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EplNRAtS-EmS"
      },
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF8DNN2U-GIZ"
      },
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sHm6QHN-HBu"
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqEz01J6-JDI"
      },
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "print('loading model...')\n",
        "hub_model = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/d7/1\")\n",
        "print('model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnebdV12-L8s"
      },
      "source": [
        "for game in games:\n",
        "  for img in selected_game_frames_full[game]:\n",
        "    data = []\n",
        "    data.append(img.permute(1, 2, 0).numpy())\n",
        "    image_data = np.stack(data)\n",
        "    results = hub_model(image_data)\n",
        "    result = {key:value.numpy() for key,value in results.items()}\n",
        "    label_id_offset = 0\n",
        "    image_np_with_detections = image_data.copy()\n",
        "\n",
        "    # Use keypoints if available in detections\n",
        "    keypoints, keypoint_scores = None, None\n",
        "    if 'detection_keypoints' in result:\n",
        "      keypoints = result['detection_keypoints'][0]\n",
        "      keypoint_scores = result['detection_keypoint_scores'][0]\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_np_with_detections[0],\n",
        "          result['detection_boxes'][0],\n",
        "          (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "          result['detection_scores'][0],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=5,\n",
        "          min_score_thresh=.05,\n",
        "          agnostic_mode=False,\n",
        "          keypoints=keypoints,\n",
        "          keypoint_scores=keypoint_scores)\n",
        "    \n",
        "    plt.figure()\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
        "    ax1.set_title('Original')\n",
        "    ax2.set_title('EfficientDet')\n",
        "    _ = ax1.imshow(img.permute(1,2,0).numpy())\n",
        "    _ = ax2.imshow(image_np_with_detections[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzfvXpHj8sZe"
      },
      "source": [
        "## Dino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oPlYVhz8tfV"
      },
      "source": [
        "def plot_attention_map_full_dino(att_map):\n",
        "    plt.figure()\n",
        "    fig, (ax1) = plt.subplots(ncols=1, figsize=(16, 16))\n",
        "    ax1.set_title('Dino Masked')\n",
        "    _ = ax1.imshow(att_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvIXDg7V8vpt"
      },
      "source": [
        "!git clone https://github.com/crimsontrigger/dino.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF0GXi2e8wsM"
      },
      "source": [
        "%cd /content/dino"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YYkfR6q8xqi"
      },
      "source": [
        "from dino.visualize_attention import (run_vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_8LZO1o8zRq"
      },
      "source": [
        "all_games = []\n",
        "for game in games:\n",
        "  ep_masked_images = run_vis(selected_game_frames_full[game])\n",
        "  all_games.append(ep_masked_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWEueXXo9F3R"
      },
      "source": [
        "for game_img in all_games:\n",
        "  for ind_game_img in game_img:\n",
        "    plot_attention_map_full_dino(ind_game_img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}